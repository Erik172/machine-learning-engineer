{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgUNO42S9x3i"
      },
      "source": [
        "# NLP con Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPUO9PylwUcr"
      },
      "source": [
        "## Procesando los datos para NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxAWeUN9zsaT"
      },
      "source": [
        "### Descargando el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b_HXFmANdVE1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrDuinME8a61"
      },
      "source": [
        "Usaremos el dataset MRPC. Este es uno de los 10 datasets que componen el [benchmark (punto de referencia) GLUE](https://huggingface.co/datasets/glue). Se utiliza para medir el rendimiento de los modelos ML en 10 tareas de clasificaci√≥n de texto diferentes.\n",
        "\n",
        "En otras palabras, seleccionamos el subset `mrpc` del dataset `glue`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sGMyV3iIdTUh"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87e22e238abb44d7940fea91d7611679",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0798a2a9723b454484b75623b2b2584c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d62d9c0c02ae4d0082ceba65388e8b55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4114c442b228451f99c9f07b008ccfab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1977a3d6eff743df890f59449ab41d21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c50326f2376a4c3192ed551dcde92bb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c758fb6f260d4c829207a0ee1593daee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('glue', 'mrpc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLRhoTyvBb_c"
      },
      "source": [
        "As√≠ se ve un ejemplo. Notamos que `mrpc` est√° compuesto de dos oraciones y una etiqueta que indica si los dos enunciados son equivalentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A7xTuFa4zegd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
              " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
              " 'label': 1,\n",
              " 'idx': 0}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = dataset['train'][0]\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t_QPFmoZBmdR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ClassLabel(names=['not_equivalent', 'equivalent'], id=None)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = dataset['train'].features['label']\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B74qToumCm-k"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'equivalent'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.int2str(example['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_At65Vv0Hmz"
      },
      "source": [
        "### Tokenizando\n",
        "\n",
        "¬øRecuerdas que con visi√≥n descargamos el feature extractor directamente del repositorio del modelo pre-entrenado que vamos a usar como base?\n",
        "\n",
        "Podemos pensar en la funci√≥n tokenizadora como el equivalente en el NLP.\n",
        "\n",
        "Descargamos el tokenizador directamente del repo del modelo que usaremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j02Vp1mo0hVq"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55002d02d76f428193fe3500635e2b9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\zer0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zer0\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "c:\\Users\\zer0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc3d35189a084ef995695cb85808e806",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a60438a4abef4300a56a84281e4ed1d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "102114067b384cef839c0b7f941e7d31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "repo_id = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUqXIm4fDi3W"
      },
      "source": [
        "Para preprocesar el conjunto de datos necesitamos convertir el texto en n√∫meros que el modelo pueda entender. Esto se hace con un tokenizador. \n",
        "\n",
        "Pasar de texto a n√∫meros se conoce como codificaci√≥n o encoding. El encoding se realiza en un proceso de dos pasos: la tokenizaci√≥n, seguida de la conversi√≥n a input ids. Por el momento nos basta saber que estamos traduciendo texto a n√∫meros llamados como input ids. Estos estar√°n en el formato adecuado para alimentar nuestro modelo.\n",
        "\n",
        "Podemos alimentar al tokenizador con una oraci√≥n o una lista de oraciones, por lo que podemos tokenizar directamente todas las primeras oraciones y todas las segundas oraciones de cada par de esta manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "huibMGKOD-Ut"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentence1 = tokenizer(example['sentence1'])\n",
        "tokenized_sentence1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBrkzA08EslP"
      },
      "source": [
        "Necesitamos manejar los dos enunciados como un par y no separados. El tokenizador puede tomar un par de secuencias y prepararlas de la manera que espera nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3O7kBG_bE18h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
              "          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
              "          3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
              "          1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
              "          2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1]])}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(example['sentence1'], example['sentence2'], return_tensors='pt')\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOzgBI7nGC31"
      },
      "source": [
        "¬øQu√© significa cada uno de los valores que nos retorna el tokenizador?\n",
        "- `input_ids` es la traducci√≥n de palabras a n√∫meros.\n",
        "- `attention_mask` es un tensor con la misma forma que `input_ids`, pero lleno de 0 y 1: los 1 indican que se debe atender a los tokens correspondientes y los 0 indican que no se deben atender. Es decir, deben ser ignorados por el modelo.\n",
        "- `token_type_ids` dice al modelo qu√© parte de la entrada es la primera oraci√≥n y cu√°l es la segunda oraci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ewj-TxGj4P"
      },
      "source": [
        "El modelo espera que las entradas sean de la forma [CLS] oraci√≥n 1 [SEP] oraci√≥n 2 [SEP] cuando hay dos oraciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w6IL1vRNjehn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'am',\n",
              " '##ro',\n",
              " '##zi',\n",
              " 'accused',\n",
              " 'his',\n",
              " 'brother',\n",
              " ',',\n",
              " 'whom',\n",
              " 'he',\n",
              " 'called',\n",
              " '\"',\n",
              " 'the',\n",
              " 'witness',\n",
              " '\"',\n",
              " ',',\n",
              " 'of',\n",
              " 'deliberately',\n",
              " 'di',\n",
              " '##stor',\n",
              " '##ting',\n",
              " 'his',\n",
              " 'evidence',\n",
              " '.',\n",
              " '[SEP]',\n",
              " 'referring',\n",
              " 'to',\n",
              " 'him',\n",
              " 'as',\n",
              " 'only',\n",
              " '\"',\n",
              " 'the',\n",
              " 'witness',\n",
              " '\"',\n",
              " ',',\n",
              " 'am',\n",
              " '##ro',\n",
              " '##zi',\n",
              " 'accused',\n",
              " 'his',\n",
              " 'brother',\n",
              " 'of',\n",
              " 'deliberately',\n",
              " 'di',\n",
              " '##stor',\n",
              " '##ting',\n",
              " 'his',\n",
              " 'evidence',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOzRDDXMG6KS"
      },
      "source": [
        "Si seleccionamos otro modelo en el Hub no necesariamente tendremos `token_type_ids` en las entradas tokenizadas (por ejemplo, no se devuelven si usa un modelo `DistilBERT`). Solo se devuelven cuando el modelo sabr√° qu√© hacer con ellas, porque los ha visto durante su preentrenamiento.\n",
        "\n",
        "En general, no necesitamos preocuparnos por si hay o no `token_type_ids` en nuestras entradas tokenizadas, siempre que usemos el tokenizador correspondiente al modelo, todo estar√° bien ya que el tokenizador sabe qu√© proporcionar al modelo.\n",
        "\n",
        "Por ejemplo, durante esta clase utilizaremos un modelo [`distilroberta-base`](https://huggingface.co/distilroberta-base) por su tama√±o y efectividad. Pero no cuenta con `token_type_ids` y a√∫n as√≠ nos regresa excelentes resultados.\n",
        "\n",
        "En la organizaci√≥n del Platzi en el Hub puedes encontrar un [modelo BERT](https://huggingface.co/platzi/platzi-distilroberta-base-mrpc-glue-omar-espejel) afinado siguiendo el mismo proceso que usamos en esta clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kUn74FigzhHm"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0785576b5a99421886ac9cb9211c2a59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\zer0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zer0\\.cache\\huggingface\\hub\\models--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "c:\\Users\\zer0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3050ab586e344059991179c7b95b068d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcf656a962d948c596edf8f2ed642ba5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b49a27530c3c4d598b5295446ed13565",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de673f22d5eb4984a347f7454a2335b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "repo_id = 'distilroberta-base'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlcHOXcQ2-kW"
      },
      "source": [
        "Creamos una funci√≥n tokenizadora. Recibe un ejemplo y lo tokeniza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1hkNcAjX0nSJ"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "49j2GVph0swg"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bff362457814ba4890b14442b59ea5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17c054f58f564a6c99995c3a65ff4ee2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02ec0f49677e48eab8501cb478416dd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prepared_dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIsjhHyf3O_L"
      },
      "source": [
        "### Definiendo el data collator: Dynamic padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIH2CWjNyQ13"
      },
      "source": [
        "Necesitamos que nuestros tensores tengan una forma rectangular. Es decir que tengan el mismo tama√±o cada uno de los ejemplos. Sin embargo, los textos no necesariamente tienen el mismo tama√±o. \n",
        "\n",
        "Para ello usamos el relleno o padding. El padding se asegura de que todas nuestras oraciones tengan la misma longitud al agregar una palabra especial llamada padding token a las oraciones con menos valores. Por ejemplo, si tenemos 10 oraciones con 10 palabras y 1 oraci√≥n con 20 palabras, el relleno garantizar√° que todas las oraciones tengan 20 palabras.\n",
        "\n",
        "Dejamos el argumento de `padding` del tokenizer vac√≠o en nuestra funci√≥n de tokenizaci√≥n por ahora. Esto se debe a que rellenar (hacer padding) todas las muestras hasta la longitud m√°xima del dataset no es eficiente, es mejor rellenar las muestras cuando estamos construyendo un batch, ya que entonces solo necesitamos rellenar hasta la longitud m√°xima en ese batch, y no la longitud m√°xima en todo el dataset. ¬°Esto puede ahorrar mucho tiempo y potencia de procesamiento cuando las entradas tienen longitudes muy variables!\n",
        "\n",
        "Usaremos un DataCollator para esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHKCmEdneI7X"
      },
      "source": [
        "Rellenemos (hagamos padding) todos los ejemplos con la longitud del elemento m√°s largo del batch. A esta t√©cnica se le conoce como relleno din√°mico o dynamic padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JMjUzWDK3SzH"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpONY8Hl3Yfb"
      },
      "source": [
        "## Entrenamiento y evaluaci√≥n\n",
        "\n",
        "Definamos el resto de los argumentos necesarios para `Trainer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlfIjQnh3bhn"
      },
      "source": [
        "### Definiendo la m√©trica "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z_h5Pm9e3ZGi"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    metric = evaluate.load('glue', 'mrpc')\n",
        "    logits, labels = p\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erd8wVZ13l-8"
      },
      "source": [
        "### Configurando `Trainer`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dy9gQdTj3q1q"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54320d031eff4ed9a24cb9b9ae8e5f32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "labels = dataset['train'].features['label'].names\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    repo_id, \n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): label for i, label in enumerate(labels)},\n",
        "    label2id={label: str(i) for i, label in enumerate(labels)}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DW93t2gG4luP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\zer0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./distilroberta-base-mrpc',\n",
        "    evaluation_strategy='steps',\n",
        "    num_train_epochs=3,\n",
        "    push_to_hub=True,\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4ek7pNKuyNJn"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=prepared_dataset['train'],\n",
        "    eval_dataset=prepared_dataset['validation'],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTZJl_b_6AvW"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fyrp6fmGy3e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37eb2b3baa184201a9b5c615cfd0d387",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1377 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics('train', train_results.metrics)\n",
        "trainer.save_metrics('train', train_results.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ-2BxDN6TjY"
      },
      "source": [
        "### Evaluaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX7GIg8b6Wsn"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate(prepared_dataset['validation'])\n",
        "trainer.log_metrics('eval', metrics)\n",
        "trainer.save_metrics('eval', metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPxunFyM6fLy"
      },
      "source": [
        "### Compartimos en el Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fowyC7_56W0S"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    'finetuned_model': model.config._name_or_path,\n",
        "    'task': 'text-classification',\n",
        "    'dataset': ['glue', 'mrpc'],\n",
        "    'tags': ['text-classification', 'glue', 'mrpc'],\n",
        "    'notes': 'Fine-tuned on GLUE MRPC dataset',\n",
        "}\n",
        "\n",
        "trainer.push_to_hub(**kwargs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
