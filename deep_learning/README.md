# üß† Deep Learning

Esta carpeta contiene implementaciones y apuntes de algoritmos y arquitecturas de Deep Learning, desde fundamentos hasta t√©cnicas avanzadas.

## üìÅ Estructura de Contenidos

### ‚ö° Fundamentos
- **[‚ö° Gradient Descent](gradient_descent/)** - Optimizaci√≥n y backpropagation
  - Algoritmos de optimizaci√≥n (SGD, Adam, RMSprop)
  - Backpropagation paso a paso
  - Learning rate scheduling
  - Momentum y t√©cnicas de aceleraci√≥n

- **[üï∏Ô∏è Neural Networks](neuronal_networks/)** - Redes neuronales b√°sicas
  - Perceptr√≥n multicapa (MLP)
  - Funciones de activaci√≥n
  - Inicializaci√≥n de pesos
  - Regularizaci√≥n (Dropout, Batch Normalization)

### üñºÔ∏è Computer Vision
- **[üñºÔ∏è Convolutional Neural Networks](cnn/)** - CNNs fundamentales
  - Operaciones de convoluci√≥n y pooling
  - Arquitecturas cl√°sicas (LeNet, AlexNet, VGG)
  - Transfer Learning
  - Data Augmentation

- **[üèóÔ∏è Arquitecturas Avanzadas](../AlexNet.ipynb)** - Modelos espec√≠ficos
  - AlexNet - Primera CNN exitosa en ImageNet
  - ResNet - Redes residuales y skip connections
  - An√°lisis de arquitecturas modernas

### üó£Ô∏è Natural Language Processing
- **[üìù NLP Fundamentals](NLP/)** - Fundamentos de NLP con DL
  - Word embeddings (Word2Vec, GloVe)
  - Redes neuronales recurrentes (RNN, LSTM, GRU)
  - Attention mechanisms
  - Transformers b√°sicos

- **[ü§ó Hugging Face](../hugging-face/)** - Modelos preentrenados
  - BERT, GPT, T5 y otros transformers
  - Fine-tuning de modelos
  - Pipelines de NLP
  - Tokenizaci√≥n avanzada

## üõ†Ô∏è Frameworks y Herramientas

### Principales
- **TensorFlow/Keras** - Framework principal para prototipos r√°pidos
- **PyTorch** - Framework para investigaci√≥n y producci√≥n
- **Hugging Face Transformers** - Modelos de NLP preentrenados

### Utilidades
- **TensorBoard** - Visualizaci√≥n de entrenamientos
- **Weights & Biases** - Experiment tracking
- **CUDA** - Aceleraci√≥n GPU
- **Docker** - Containerizaci√≥n de modelos

## üìö Conceptos Clave Cubiertos

### Arquitecturas Fundamentales
- **Feedforward Networks** - Redes completamente conectadas
- **Convolutional Networks** - Para datos con estructura espacial
- **Recurrent Networks** - Para datos secuenciales
- **Transformer Networks** - Attention-based models

### T√©cnicas de Optimizaci√≥n
- **Gradient Descent Variants** - SGD, Adam, AdaGrad, RMSprop
- **Learning Rate Scheduling** - Decay, warm-up, cyclic
- **Regularization** - Dropout, Batch Norm, Weight Decay
- **Advanced Optimizers** - AdamW, LAMB, RAdam

### T√©cnicas Avanzadas
- **Transfer Learning** - Aprovechamiento de modelos preentrenados
- **Fine-tuning** - Adaptaci√≥n a tareas espec√≠ficas
- **Multi-task Learning** - Entrenamiento conjunto
- **Meta-learning** - Aprender a aprender

### Evaluaci√≥n y Debugging
- **Loss Functions** - Cross-entropy, MSE, custom losses
- **Metrics** - Accuracy, F1, BLEU, ROUGE
- **Visualization** - Feature maps, attention weights
- **Debugging** - Gradient flow, overfitting detection

## üéØ Objetivos de Aprendizaje

Al completar esta secci√≥n, deber√≠as ser capaz de:

- [ ] Implementar redes neuronales desde cero
- [ ] Usar TensorFlow/Keras y PyTorch efectivamente
- [ ] Dise√±ar arquitecturas para problemas espec√≠ficos
- [ ] Aplicar transfer learning y fine-tuning
- [ ] Optimizar hiperpar√°metros y debugging
- [ ] Implementar modelos state-of-the-art
- [ ] Desplegar modelos en producci√≥n

## üìä Datasets Utilizados

### Computer Vision
- **MNIST** - D√≠gitos escritos a mano
- **CIFAR-10/100** - Clasificaci√≥n de objetos
- **ImageNet** - Dataset masivo de im√°genes
- **COCO** - Detecci√≥n y segmentaci√≥n

### Natural Language Processing
- **IMDB** - An√°lisis de sentimientos
- **WikiText** - Modelado de lenguaje
- **SQuAD** - Question answering
- **GLUE** - Benchmark de NLP

## üöÄ Proyectos Pr√°cticos

### Nivel Principiante
1. **Clasificador de d√≠gitos MNIST** con MLP
2. **Clasificador CIFAR-10** con CNN b√°sica
3. **An√°lisis de sentimientos** con RNN

### Nivel Intermedio
1. **Transfer learning** con modelos preentrenados
2. **Generador de texto** con LSTM
3. **Autoencoder** para reducci√≥n de dimensionalidad

### Nivel Avanzado
1. **Implementaci√≥n de ResNet** desde cero
2. **Fine-tuning de BERT** para clasificaci√≥n
3. **GAN** para generaci√≥n de im√°genes

## üìñ Recursos Recomendados

### Libros
- "Deep Learning" - Ian Goodfellow, Yoshua Bengio, Aaron Courville
- "Deep Learning with Python" - Fran√ßois Chollet
- "Hands-On Machine Learning" - Aur√©lien G√©ron

### Cursos
- CS231n: Convolutional Neural Networks (Stanford)
- CS224n: Natural Language Processing (Stanford)
- Fast.ai Practical Deep Learning for Coders

### Papers Fundamentales
- "ImageNet Classification with Deep CNNs" (AlexNet)
- "Deep Residual Learning for Image Recognition" (ResNet)
- "Attention Is All You Need" (Transformer)
- "BERT: Pre-training of Deep Bidirectional Transformers"

## üîß Setup y Configuraci√≥n

### Instalaci√≥n B√°sica
```bash
# TensorFlow
pip install tensorflow

# PyTorch
pip install torch torchvision torchaudio

# Hugging Face
pip install transformers datasets

# Utilidades
pip install wandb tensorboard matplotlib seaborn
```

### GPU Setup
```bash
# Verificar GPU disponible
python -c "import torch; print(torch.cuda.is_available())"
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
```

## üö® Consideraciones Importantes

### Recursos Computacionales
- **GPU recomendada** para entrenamientos eficientes
- **Google Colab** para experimentaci√≥n gratuita
- **Kaggle Kernels** como alternativa
- **Cloud platforms** para proyectos grandes

### Mejores Pr√°cticas
- **Versionado de experimentos** con MLflow/W&B
- **Reproducibilidad** con seeds fijos
- **Monitoreo** de m√©tricas durante entrenamiento
- **Checkpointing** para entrenamientos largos

## üîÑ Flujo de Trabajo T√≠pico

1. **Exploraci√≥n de datos** y an√°lisis inicial
2. **Preprocesamiento** y augmentaci√≥n
3. **Dise√±o de arquitectura** o selecci√≥n de modelo
4. **Entrenamiento** con validaci√≥n
5. **Evaluaci√≥n** y an√°lisis de resultados
6. **Fine-tuning** e iteraci√≥n
7. **Deployment** y monitoreo

---

üí° **Tip**: El Deep Learning requiere mucha experimentaci√≥n. Mant√©n un registro detallado de tus experimentos y no tengas miedo de iterar m√∫ltiples veces. 